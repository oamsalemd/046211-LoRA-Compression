{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import optuna\n",
    "import detectors\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Append local modules to 'sys.path':\n",
    "proj_path = \"/home/ohada/DeepProject/ProjectPath\"\n",
    "if proj_path not in sys.path:\n",
    "    sys.path.append(proj_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA RTX A5000'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the names all available GPU devices:\n",
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get the CIFAR10 dataset from 'torch':\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download the training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Create data loaders:\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet18_cifar10\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step 0 the accuracy is 95.3125%\n",
      "For step 10 the accuracy is 94.88636363636364%\n",
      "For step 20 the accuracy is 95.38690476190476%\n",
      "For step 30 the accuracy is 95.2116935483871%\n",
      "For step 40 the accuracy is 94.74085365853658%\n",
      "For step 50 the accuracy is 94.79166666666667%\n",
      "For step 60 the accuracy is 94.82581967213115%\n",
      "For step 70 the accuracy is 94.93838028169014%\n",
      "For step 80 the accuracy is 94.86882716049382%\n",
      "For step 90 the accuracy is 94.86607142857143%\n",
      "For step 100 the accuracy is 94.87933168316832%\n",
      "For step 110 the accuracy is 94.80574324324324%\n",
      "For step 120 the accuracy is 94.86053719008264%\n",
      "For step 130 the accuracy is 95.00238549618321%\n",
      "For step 140 the accuracy is 95.06870567375887%\n",
      "For step 150 the accuracy is 94.98137417218543%\n",
      "Accuracy of the network on the 10000 test images: 94.98%\n"
     ]
    }
   ],
   "source": [
    "# calculate the model's accuracy on the test data:\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# CIFAR10 mean and standard deviation:\n",
    "mean = [       0.4914,      0.4822,      0.4465    ]\n",
    "std = [      0.2023,      0.1994,      0.201 ]\n",
    "#mean = [       0.5,      0.5,      0.5    ]\n",
    "#std = [      0.5,      0.5,      0.5 ]\n",
    "\n",
    "normalize = transforms.Normalize(mean, std)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        images, labels = data\n",
    "        # Normalize the images batch:\n",
    "        images = normalize(images)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'For step {i} the accuracy is {100 * correct / total}%')\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def quant_tensor(tensor, size=4*4):\n",
    "    quant_model_flatten = tensor\n",
    "    if tensor.numel() < size:\n",
    "        size = tensor.numel()\n",
    "\n",
    "    orig_shape = quant_model_flatten.shape\n",
    "    quant_model_flatten = quant_model_flatten.flatten()\n",
    "    # For each 'size' subsection, keep the maximum value, and set the rest to zero:\n",
    "    for i in range(0, len(quant_model_flatten), size):\n",
    "        # Get the maximum value in the subsection:\n",
    "        max_val = quant_model_flatten[i:i+size].max()\n",
    "        max_index = quant_model_flatten[i:i+size].argmax()\n",
    "        # Set the subsection to zero:\n",
    "        quant_model_flatten[i:i+size] = 0\n",
    "        # Restore only 'max_val' to 'max_index' in the subsection:\n",
    "        quant_model_flatten[i+max_index] = max_val\n",
    "\n",
    "    quant_model_flatten = quant_model_flatten.view(orig_shape)\n",
    "\n",
    "    return quant_model_flatten\n",
    "\n",
    "def quantize_linear_layers(model, size=4*4):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            # Extract weights and biases\n",
    "            weight = module.weight\n",
    "            bias = module.bias\n",
    "\n",
    "            # Quantize weights\n",
    "            quantized_weight = quant_tensor(weight, size)\n",
    "\n",
    "            # Replace original weights with quantized weights\n",
    "            module.weight = nn.Parameter(quantized_weight, requires_grad=False)\n",
    "\n",
    "            # Optional: Quantize biases if they exist\n",
    "            quantized_bias = quant_tensor(bias)\n",
    "            module.bias = nn.Parameter(quantized_bias, requires_grad=False)\n",
    "    return model\n",
    "\n",
    "quant_model = quantize_linear_layers(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step 0 the accuracy is 34.375%\n",
      "For step 10 the accuracy is 25.568181818181817%\n",
      "For step 20 the accuracy is 26.785714285714285%\n",
      "For step 30 the accuracy is 27.52016129032258%\n",
      "For step 40 the accuracy is 28.201219512195124%\n",
      "For step 50 the accuracy is 28.61519607843137%\n",
      "For step 60 the accuracy is 28.50922131147541%\n",
      "For step 70 the accuracy is 28.763204225352112%\n",
      "For step 80 the accuracy is 28.53009259259259%\n",
      "For step 90 the accuracy is 28.537087912087912%\n",
      "For step 100 the accuracy is 28.836633663366335%\n",
      "For step 110 the accuracy is 28.95551801801802%\n",
      "For step 120 the accuracy is 28.873966942148762%\n",
      "For step 130 the accuracy is 28.924141221374047%\n",
      "For step 140 the accuracy is 28.97828014184397%\n",
      "For step 150 the accuracy is 28.756208609271525%\n",
      "Accuracy of the quantized network on the 10000 test images: 28.74%\n"
     ]
    }
   ],
   "source": [
    "# calculate the model's accuracy on the test data:\n",
    "quant_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# CIFAR10 mean and standard deviation:\n",
    "mean = [       0.4914,      0.4822,      0.4465    ]\n",
    "std = [      0.2023,      0.1994,      0.201 ]\n",
    "#mean = [       0.5,      0.5,      0.5    ]\n",
    "#std = [      0.5,      0.5,      0.5 ]\n",
    "\n",
    "normalize = transforms.Normalize(mean, std)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        images, labels = data\n",
    "        # Normalize the images batch:\n",
    "        images = normalize(images)\n",
    "        outputs = quant_model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        if i % 10 == 0:\n",
    "            print(f'For step {i} the accuracy is {100 * correct / total}%')\n",
    "\n",
    "print(f'Accuracy of the quantized network on the 10000 test images: {100 * correct / total}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA implementation and tests over MLP module and VGG pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "def ReplaceLinearToLoRA(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank=rank, alpha=alpha))\n",
    "        else:\n",
    "            ReplaceLinearToLoRA(module, rank=rank, alpha=alpha)\n",
    "\n",
    "def FreeazeModel(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "def UnfreezeLoRA(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, LoRALayer):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            UnfreezeLoRA(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "        num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MultilayerPerceptron(\n",
    "    num_features=100,\n",
    "    num_hidden_1=1000,\n",
    "    num_hidden_2=1000, \n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=100, out_features=1000, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ReplaceLinearToLoRA(model, 4, 8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreeazeModel(model)\n",
    "UnfreezeLoRA(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplaceLinearToLoRA(vgg16_model, 4, 8)\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreeazeModel(vgg16_model)\n",
    "UnfreezeLoRA(vgg16_model)\n",
    "for name, param in vgg16_model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/4], Step [5000/5000], Loss: nan, Accuracy: 10.05%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [2/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [3/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [4/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:05:09,003] Trial 0 failed with parameters: {'lora_alpha': 40, 'optimizer': 'SGD', 'lr': 0.021811288119580597, 'batch_size': 8, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:05:09,004] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/3], Step [625/625], Loss: nan, Accuracy: 10.27%\n",
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Epoch [2/3], Step [625/625], Loss: nan, Accuracy: 10.08%\n",
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Epoch [3/3], Step [625/625], Loss: nan, Accuracy: 10.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:05:33,624] Trial 1 failed with parameters: {'lora_alpha': 24, 'optimizer': 'SGD', 'lr': 0.19912349713389108, 'batch_size': 64, 'num_epochs': 3} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:05:33,625] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/6], Step [625/625], Loss: nan, Accuracy: 10.62%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [2/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [3/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [4/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [5/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [6/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:06:19,338] Trial 2 failed with parameters: {'lora_alpha': 48, 'optimizer': 'SGD', 'lr': 0.015138820653668743, 'batch_size': 64, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:06:19,338] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 0.1714, Accuracy: 96.17%\n",
      "Val Loss: 0.1014, Val Accuracy: 98.30%\n",
      "Epoch [2/5], Step [625/625], Loss: 0.1740, Accuracy: 96.13%\n",
      "Val Loss: 0.6072, Val Accuracy: 83.27%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1705, Accuracy: 96.27%\n",
      "Val Loss: 0.0746, Val Accuracy: 98.60%\n",
      "Epoch [4/5], Step [625/625], Loss: 0.1159, Accuracy: 97.70%\n",
      "Val Loss: 0.1325, Val Accuracy: 97.53%\n",
      "Epoch [5/5], Step [625/625], Loss: 0.2009, Accuracy: 95.79%\n",
      "Val Loss: 1.3934, Val Accuracy: 69.71%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/4], Step [5000/5000], Loss: 2.2882, Accuracy: 73.38%\n",
      "Val Loss: 5.2116, Val Accuracy: 46.69%\n",
      "Epoch [2/4], Step [5000/5000], Loss: 3.0742, Accuracy: 74.94%\n",
      "Val Loss: 0.9900, Val Accuracy: 79.88%\n",
      "Epoch [3/4], Step [5000/5000], Loss: 4.1397, Accuracy: 74.81%\n",
      "Val Loss: 0.4905, Val Accuracy: 86.07%\n",
      "Epoch [4/4], Step [5000/5000], Loss: 2.8908, Accuracy: 76.80%\n",
      "Val Loss: 8.6441, Val Accuracy: 31.10%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/2], Step [5000/5000], Loss: 0.6368, Accuracy: 85.10%\n",
      "Val Loss: 0.2694, Val Accuracy: 92.80%\n",
      "Epoch [2/2], Step [5000/5000], Loss: 0.4458, Accuracy: 89.37%\n",
      "Val Loss: 0.1610, Val Accuracy: 97.74%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 1.3562, Accuracy: 88.12%\n",
      "Val Loss: 0.7727, Val Accuracy: 80.60%\n",
      "Epoch [2/5], Step [625/625], Loss: 0.1869, Accuracy: 96.03%\n",
      "Val Loss: 0.0759, Val Accuracy: 98.52%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1120, Accuracy: 97.62%\n",
      "Val Loss: 0.0873, Val Accuracy: 98.23%\n",
      "Epoch [4/5], Step [625/625], Loss: 3.7163, Accuracy: 85.12%\n",
      "Val Loss: 4.5051, Val Accuracy: 67.45%\n",
      "Epoch [5/5], Step [625/625], Loss: 0.3836, Accuracy: 94.20%\n",
      "Val Loss: 0.0671, Val Accuracy: 98.75%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/6], Step [1250/1250], Loss: 0.2938, Accuracy: 92.96%\n",
      "Val Loss: 0.1431, Val Accuracy: 97.57%\n",
      "Epoch [2/6], Step [1250/1250], Loss: 0.3498, Accuracy: 92.46%\n",
      "Val Loss: 0.1081, Val Accuracy: 98.10%\n",
      "Epoch [3/6], Step [1250/1250], Loss: 0.5047, Accuracy: 91.72%\n",
      "Val Loss: 0.0876, Val Accuracy: 98.48%\n",
      "Epoch [4/6], Step [1250/1250], Loss: 0.1556, Accuracy: 96.62%\n",
      "Val Loss: 0.1054, Val Accuracy: 98.28%\n",
      "Epoch [5/6], Step [1250/1250], Loss: 0.7857, Accuracy: 89.82%\n",
      "Val Loss: 0.1255, Val Accuracy: 97.84%\n",
      "Epoch [6/6], Step [1250/1250], Loss: 0.1423, Accuracy: 97.01%\n",
      "Val Loss: 0.0813, Val Accuracy: 98.57%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/7], Step [1250/1250], Loss: nan, Accuracy: 9.91%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [2/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [3/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [4/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [5/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [6/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [7/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:11:41,656] Trial 8 failed with parameters: {'lora_alpha': 48, 'optimizer': 'SGD', 'lr': 0.0634383447468684, 'batch_size': 32, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:11:41,657] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 13.6492, Accuracy: 53.09%\n",
      "Val Loss: 2.0995, Val Accuracy: 76.05%\n",
      "Epoch [2/5], Step [625/625], Loss: 5.9190, Accuracy: 60.47%\n",
      "Val Loss: 0.3330, Val Accuracy: 92.08%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1708, Accuracy: 96.05%\n",
      "Val Loss: 0.0850, Val Accuracy: 98.36%\n",
      "Epoch [4/5], Step [625/625], Loss: 0.1408, Accuracy: 96.97%\n",
      "Val Loss: 0.0874, Val Accuracy: 98.60%\n",
      "Epoch [5/5], Step [625/625], Loss: 58.5076, Accuracy: 82.99%\n",
      "Val Loss: 41.9884, Val Accuracy: 65.85%\n",
      "Study statistics: \n",
      " Number of finished trials:  10\n",
      " Number of pruned trials:  0\n",
      " Number of complete trials:  6\n",
      "Best trial:\n",
      " Value:  41.98835474974031\n",
      " Params: \n",
      " lora_alpha: 48\n",
      " optimizer: Adam\n",
      " lr: 0.16628497525202182\n",
      " batch_size: 64\n",
      " num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "from train_evaluate.train_model import *\n",
    "study = optuna_trials()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For 'mean' quant, 'rank'=10:\n",
    "\n",
    "Study statistics:\n",
    " Number of finished trials:  10\n",
    " Number of pruned trials:  0\n",
    " Number of complete trials:  9\n",
    "Best trial:\n",
    " Value:  29.649906891629023\n",
    " Params:\n",
    " lora_alpha: 32\n",
    " optimizer: Adam\n",
    " lr: 0.08913593530369923\n",
    " batch_size: 32\n",
    " num_epochs: 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['vgg16_bn_cifar10', 'vgg16_bn_cifar100']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in timm.list_models(pretrained=True) if 'vgg' in i and 'cifar' in i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/edadaltocg/vgg16_bn_cifar10/resolve/main/pytorch_model.bin\" to /home/ohada/.cache/torch/hub/checkpoints/vgg16_bn_cifar10.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 56.2M/56.2M [00:02<00:00, 29.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "m_vgg = timm.create_model(\"vgg16_bn_cifar10\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['repvgg_a0.rvgg_in1k',\n 'repvgg_a1.rvgg_in1k',\n 'repvgg_a2.rvgg_in1k',\n 'repvgg_b0.rvgg_in1k',\n 'repvgg_b1.rvgg_in1k',\n 'repvgg_b1g4.rvgg_in1k',\n 'repvgg_b2.rvgg_in1k',\n 'repvgg_b2g4.rvgg_in1k',\n 'repvgg_b3.rvgg_in1k',\n 'repvgg_b3g4.rvgg_in1k',\n 'repvgg_d2se.rvgg_in1k',\n 'vgg11.tv_in1k',\n 'vgg11_bn.tv_in1k',\n 'vgg13.tv_in1k',\n 'vgg13_bn.tv_in1k',\n 'vgg16.tv_in1k',\n 'vgg16_bn.tv_in1k',\n 'vgg16_bn_cifar10',\n 'vgg16_bn_cifar100',\n 'vgg16_bn_svhn',\n 'vgg19.tv_in1k',\n 'vgg19_bn.tv_in1k']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in timm.list_models(pretrained=True) if 'vgg' in i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/edadaltocg/densenet121_cifar10/resolve/main/pytorch_model.bin\" to /home/ohada/.cache/torch/hub/checkpoints/densenet121_cifar10.pth\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4.19M/4.19M [00:00<00:00, 6.34MB/s]\n"
     ]
    }
   ],
   "source": [
    "m_dense = timm.create_model(\"densenet121_cifar10\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2570473ccdd343c0a4b43bb8e9a99caf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of 'Linear' layers in the model:\n",
    "def count_linear_layers(model):\n",
    "    count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "count_linear_layers(timm.create_model(\"vgg16.tv_in1k\", pretrained=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "m_vgg_1k = timm.create_model(\"vgg16.tv_in1k\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (pre_logits): ConvMlp(\n    (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n    (act1): ReLU(inplace=True)\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n    (act2): ReLU(inplace=True)\n  )\n  (head): ClassifierHead(\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc): Linear(in_features=4096, out_features=1000, bias=True)\n    (flatten): Identity()\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
