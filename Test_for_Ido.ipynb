{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "import optuna\n",
    "import detectors\n",
    "import timm\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "from torchvision import datasets\n",
    "# %matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "# Append local modules to 'sys.path':\n",
    "proj_path = \"/home/ohada/DeepProject/ProjectPath\"\n",
    "if proj_path not in sys.path:\n",
    "    sys.path.append(proj_path)\n",
    "\n",
    "from train_evaluate.quant_func import *\n",
    "from helper_functions.quant_lora import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'NVIDIA RTX A5000'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# show the names all available GPU devices:\n",
    "[torch.cuda.device(i) for i in range(torch.cuda.device_count())]\n",
    "torch.cuda.device_count()\n",
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Get the CIFAR10 dataset from 'torch':\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download the training data from open datasets.\n",
    "training_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "test_data = datasets.CIFAR10(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor()\n",
    ")\n",
    "\n",
    "# Create data loaders:\n",
    "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = timm.create_model(\"resnet18_cifar10\", pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step 0 the accuracy is 92.1875%\n",
      "For step 1 the accuracy is 89.0625%\n",
      "For step 2 the accuracy is 91.66666666666667%\n",
      "For step 3 the accuracy is 93.359375%\n",
      "For step 4 the accuracy is 93.75%\n",
      "For step 5 the accuracy is 94.53125%\n",
      "For step 6 the accuracy is 94.64285714285714%\n",
      "For step 7 the accuracy is 94.921875%\n",
      "For step 8 the accuracy is 94.61805555555556%\n",
      "For step 9 the accuracy is 94.53125%\n",
      "For step 10 the accuracy is 94.60227272727273%\n",
      "For step 11 the accuracy is 94.40104166666667%\n",
      "For step 12 the accuracy is 94.35096153846153%\n",
      "For step 13 the accuracy is 94.30803571428571%\n",
      "For step 14 the accuracy is 94.47916666666667%\n",
      "For step 15 the accuracy is 94.43359375%\n",
      "For step 16 the accuracy is 94.48529411764706%\n",
      "For step 17 the accuracy is 94.44444444444444%\n",
      "For step 18 the accuracy is 94.32565789473684%\n",
      "For step 19 the accuracy is 94.375%\n",
      "For step 20 the accuracy is 94.3452380952381%\n",
      "For step 21 the accuracy is 94.10511363636364%\n",
      "For step 22 the accuracy is 94.02173913043478%\n",
      "For step 23 the accuracy is 93.88020833333333%\n",
      "For step 24 the accuracy is 93.875%\n",
      "For step 25 the accuracy is 93.93028846153847%\n",
      "For step 26 the accuracy is 93.92361111111111%\n",
      "For step 27 the accuracy is 93.80580357142857%\n",
      "For step 28 the accuracy is 93.80387931034483%\n",
      "For step 29 the accuracy is 93.64583333333333%\n",
      "For step 30 the accuracy is 93.6491935483871%\n",
      "For step 31 the accuracy is 93.65234375%\n",
      "For step 32 the accuracy is 93.65530303030303%\n",
      "For step 33 the accuracy is 93.65808823529412%\n",
      "For step 34 the accuracy is 93.61607142857143%\n",
      "For step 35 the accuracy is 93.57638888888889%\n",
      "For step 36 the accuracy is 93.70777027027027%\n",
      "For step 37 the accuracy is 93.62664473684211%\n",
      "For step 38 the accuracy is 93.7099358974359%\n",
      "For step 39 the accuracy is 93.671875%\n",
      "For step 40 the accuracy is 93.67378048780488%\n",
      "For step 41 the accuracy is 93.67559523809524%\n",
      "For step 42 the accuracy is 93.6046511627907%\n",
      "For step 43 the accuracy is 93.67897727272727%\n",
      "For step 44 the accuracy is 93.61111111111111%\n",
      "For step 45 the accuracy is 93.64809782608695%\n",
      "For step 46 the accuracy is 93.6502659574468%\n",
      "For step 47 the accuracy is 93.75%\n",
      "For step 48 the accuracy is 93.75%\n",
      "For step 49 the accuracy is 93.84375%\n",
      "For step 50 the accuracy is 93.87254901960785%\n",
      "For step 51 the accuracy is 93.96033653846153%\n",
      "For step 52 the accuracy is 93.98584905660377%\n",
      "For step 53 the accuracy is 93.92361111111111%\n",
      "For step 54 the accuracy is 93.92045454545455%\n",
      "For step 55 the accuracy is 93.9453125%\n",
      "For step 56 the accuracy is 93.94188596491227%\n",
      "For step 57 the accuracy is 93.88469827586206%\n",
      "For step 58 the accuracy is 93.88241525423729%\n",
      "For step 59 the accuracy is 93.90625%\n",
      "For step 60 the accuracy is 93.87807377049181%\n",
      "For step 61 the accuracy is 93.8508064516129%\n",
      "For step 62 the accuracy is 93.84920634920636%\n",
      "For step 63 the accuracy is 93.896484375%\n",
      "For step 64 the accuracy is 93.91826923076923%\n",
      "For step 65 the accuracy is 93.96306818181819%\n",
      "For step 66 the accuracy is 93.9598880597015%\n",
      "For step 67 the accuracy is 93.93382352941177%\n",
      "For step 68 the accuracy is 93.86322463768116%\n",
      "For step 69 the accuracy is 93.81696428571429%\n",
      "For step 70 the accuracy is 93.75%\n",
      "For step 71 the accuracy is 93.72829861111111%\n",
      "For step 72 the accuracy is 93.75%\n",
      "For step 73 the accuracy is 93.75%\n",
      "For step 74 the accuracy is 93.70833333333333%\n",
      "For step 75 the accuracy is 93.66776315789474%\n",
      "For step 76 the accuracy is 93.66883116883118%\n",
      "For step 77 the accuracy is 93.7099358974359%\n",
      "For step 78 the accuracy is 93.76977848101266%\n",
      "For step 79 the accuracy is 93.76953125%\n",
      "For step 80 the accuracy is 93.75%\n",
      "For step 81 the accuracy is 93.71189024390245%\n",
      "For step 82 the accuracy is 93.71234939759036%\n",
      "For step 83 the accuracy is 93.67559523809524%\n",
      "For step 84 the accuracy is 93.69485294117646%\n",
      "For step 85 the accuracy is 93.65915697674419%\n",
      "For step 86 the accuracy is 93.69612068965517%\n",
      "For step 87 the accuracy is 93.71448863636364%\n",
      "For step 88 the accuracy is 93.73244382022472%\n",
      "For step 89 the accuracy is 93.71527777777777%\n",
      "For step 90 the accuracy is 93.69848901098901%\n",
      "For step 91 the accuracy is 93.73301630434783%\n",
      "For step 92 the accuracy is 93.76680107526882%\n",
      "For step 93 the accuracy is 93.76662234042553%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[43], line 16\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtqdm\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tqdm\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m---> 16\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(test_dataloader):\n\u001B[1;32m     17\u001B[0m         images, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m     18\u001B[0m         \u001B[38;5;66;03m# Normalize the images batch:\u001B[39;00m\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:51\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__(possibly_batched_index)\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     53\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[possibly_batched_index]\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/datasets/cifar.py:118\u001B[0m, in \u001B[0;36mCIFAR10.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    115\u001B[0m img \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mfromarray(img)\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 118\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    121\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:137\u001B[0m, in \u001B[0;36mToTensor.__call__\u001B[0;34m(self, pic)\u001B[0m\n\u001B[1;32m    129\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, pic):\n\u001B[1;32m    130\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;124;03m        Tensor: Converted image.\u001B[39;00m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 137\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:143\u001B[0m, in \u001B[0;36mto_tensor\u001B[0;34m(pic)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (F_pil\u001B[38;5;241m.\u001B[39m_is_pil_image(pic) \u001B[38;5;129;01mor\u001B[39;00m _is_numpy(pic)):\n\u001B[1;32m    141\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be PIL Image or ndarray. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(pic)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 143\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43m_is_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpic\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_numpy_image(pic):\n\u001B[1;32m    144\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpic should be 2/3 dimensional. Got \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mpic\u001B[38;5;241m.\u001B[39mndim\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m dimensions.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    146\u001B[0m default_float_dtype \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mget_default_dtype()\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:118\u001B[0m, in \u001B[0;36m_is_numpy\u001B[0;34m(img)\u001B[0m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;129m@torch\u001B[39m\u001B[38;5;241m.\u001B[39mjit\u001B[38;5;241m.\u001B[39munused\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_is_numpy\u001B[39m(img: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mbool\u001B[39m:\n\u001B[0;32m--> 118\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43misinstance\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndarray\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# calculate the model's accuracy on the test data:\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# CIFAR10 mean and standard deviation:\n",
    "mean = [       0.4914,      0.4822,      0.4465    ]\n",
    "std = [      0.2023,      0.1994,      0.201 ]\n",
    "#mean = [       0.5,      0.5,      0.5    ]\n",
    "#std = [      0.5,      0.5,      0.5 ]\n",
    "\n",
    "normalize = transforms.Normalize(mean, std)\n",
    "from tqdm import tqdm\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, data in enumerate(test_dataloader):\n",
    "        images, labels = data\n",
    "        # Normalize the images batch:\n",
    "        images = normalize(images)\n",
    "        outputs = model(images)\n",
    "        predicted = outputs.argmax(dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        tqdm.write(f'For step {i} the accuracy is {100 * correct / total}%')\n",
    "        # if i % 10 == 0:\n",
    "        #     print(f'For step {i} the accuracy is {100 * correct / total}%')\n",
    "\n",
    "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/46.8M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff051f1cd2ea47f98172df3cf26c2b81"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "m_resnet = timm.create_model(\"resnet18.tv_in1k\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LoRA implementation and tests over MLP module and VGG pretrained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step 0 the accuracy is 73.4375%\n",
      "For step 10 the accuracy is 68.89204545454545%\n",
      "For step 20 the accuracy is 69.7172619047619%\n",
      "For step 30 the accuracy is 70.4133064516129%\n",
      "For step 40 the accuracy is 70.84603658536585%\n",
      "For step 50 the accuracy is 70.89460784313725%\n",
      "For step 60 the accuracy is 70.51741803278688%\n",
      "For step 70 the accuracy is 70.77464788732394%\n",
      "For step 80 the accuracy is 70.44753086419753%\n",
      "For step 90 the accuracy is 70.29532967032966%\n",
      "For step 100 the accuracy is 70.18873762376238%\n",
      "For step 110 the accuracy is 70.27027027027027%\n",
      "For step 120 the accuracy is 70.26084710743801%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[46], line 8\u001B[0m\n\u001B[1;32m      5\u001B[0m device \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mcuda\u001B[38;5;241m.\u001B[39mis_available() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m      7\u001B[0m m_resnet \u001B[38;5;241m=\u001B[39m m_resnet\u001B[38;5;241m.\u001B[39mto(device)\n\u001B[0;32m----> 8\u001B[0m \u001B[43mevaluate_test\u001B[49m\u001B[43m(\u001B[49m\u001B[43mm_resnet\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtest_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DeepProject/ProjectPath/train_evaluate/train_model.py:167\u001B[0m, in \u001B[0;36mevaluate_test\u001B[0;34m(model, test_loader, device)\u001B[0m\n\u001B[1;32m    164\u001B[0m total \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[1;32m    166\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 167\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i, data \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(test_loader):\n\u001B[1;32m    168\u001B[0m         \u001B[38;5;66;03m# Move to device:\u001B[39;00m\n\u001B[1;32m    169\u001B[0m         images, labels \u001B[38;5;241m=\u001B[39m data\n\u001B[1;32m    170\u001B[0m         images, labels \u001B[38;5;241m=\u001B[39m images\u001B[38;5;241m.\u001B[39mto(device), labels\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    628\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    629\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[1;32m    630\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 631\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    632\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m    633\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[1;32m    635\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:675\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    673\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    674\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m--> 675\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m    676\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[1;32m    677\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[1;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[0;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[0;34m(self, indices)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/utils/data/dataset.py:399\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    397\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[1;32m    398\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 399\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/datasets/folder.py:231\u001B[0m, in \u001B[0;36mDatasetFolder.__getitem__\u001B[0;34m(self, index)\u001B[0m\n\u001B[1;32m    229\u001B[0m sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mloader(path)\n\u001B[1;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 231\u001B[0m     sample \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43msample\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    232\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    233\u001B[0m     target \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget_transform(target)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[0;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/transforms.py:354\u001B[0m, in \u001B[0;36mResize.forward\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[1;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    348\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[1;32m    349\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    352\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 354\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/functional.py:467\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[1;32m    465\u001B[0m         warnings\u001B[38;5;241m.\u001B[39mwarn(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    466\u001B[0m     pil_interpolation \u001B[38;5;241m=\u001B[39m pil_modes_mapping[interpolation]\n\u001B[0;32m--> 467\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_pil\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpil_interpolation\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    469\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_t\u001B[38;5;241m.\u001B[39mresize(img, size\u001B[38;5;241m=\u001B[39moutput_size, interpolation\u001B[38;5;241m=\u001B[39minterpolation\u001B[38;5;241m.\u001B[39mvalue, antialias\u001B[38;5;241m=\u001B[39mantialias)\n",
      "File \u001B[0;32m~/DeepProject/DeepProject/ProjectPath/venv/lib/python3.10/site-packages/torchvision/transforms/_functional_pil.py:250\u001B[0m, in \u001B[0;36mresize\u001B[0;34m(img, size, interpolation)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mlist\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m):\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGot inappropriate size arg: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00msize\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 250\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mimg\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mtuple\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m[\u001B[49m\u001B[43m:\u001B[49m\u001B[43m:\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minterpolation\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3/dist-packages/PIL/Image.py:1980\u001B[0m, in \u001B[0;36mImage.resize\u001B[0;34m(self, size, resample, box, reducing_gap)\u001B[0m\n\u001B[1;32m   1972\u001B[0m             \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m Image\u001B[38;5;241m.\u001B[39mreduce(\u001B[38;5;28mself\u001B[39m, factor, box\u001B[38;5;241m=\u001B[39mreduce_box)\n\u001B[1;32m   1973\u001B[0m         box \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   1974\u001B[0m             (box[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   1975\u001B[0m             (box[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   1976\u001B[0m             (box[\u001B[38;5;241m2\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m0\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_x,\n\u001B[1;32m   1977\u001B[0m             (box[\u001B[38;5;241m3\u001B[39m] \u001B[38;5;241m-\u001B[39m reduce_box[\u001B[38;5;241m1\u001B[39m]) \u001B[38;5;241m/\u001B[39m factor_y,\n\u001B[1;32m   1978\u001B[0m         )\n\u001B[0;32m-> 1980\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_new(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mim\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mresample\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbox\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from train_evaluate.train_model import *\n",
    "_, test_loader = get_imagenet_data(batch_size=64, loader=True)\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "m_resnet = m_resnet.to(device)\n",
    "evaluate_test(m_resnet, test_loader, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "quantize_linear_layers(m_resnet, 'sparse')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For step 0 the accuracy is 54.6875%\n",
      "For step 10 the accuracy is 55.25568181818182%\n",
      "For step 20 the accuracy is 54.38988095238095%\n",
      "For step 30 the accuracy is 55.443548387096776%\n",
      "For step 40 the accuracy is 54.99237804878049%\n",
      "For step 50 the accuracy is 55.05514705882353%\n",
      "For step 60 the accuracy is 54.63627049180328%\n",
      "For step 70 the accuracy is 54.86355633802817%\n",
      "For step 80 the accuracy is 55.15046296296296%\n",
      "For step 90 the accuracy is 55.511675824175825%\n",
      "For step 100 the accuracy is 55.522896039603964%\n",
      "For step 110 the accuracy is 55.560247747747745%\n",
      "For step 120 the accuracy is 55.617252066115704%\n",
      "For step 130 the accuracy is 55.55820610687023%\n",
      "For step 140 the accuracy is 55.57402482269504%\n",
      "For step 150 the accuracy is 55.308360927152314%\n",
      "Accuracy of the network on the 10000 test images: 55.32%\n"
     ]
    }
   ],
   "source": [
    "evaluate_test(m_resnet, test_loader, device)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16_model = torchvision.models.vgg16(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRALayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, rank, alpha):\n",
    "        super().__init__()\n",
    "        std_dev = 1 / torch.sqrt(torch.tensor(rank).float())\n",
    "        self.A = nn.Parameter(torch.randn(in_dim, rank) * std_dev)\n",
    "        self.B = nn.Parameter(torch.zeros(rank, out_dim))\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.alpha * (x @ self.A @ self.B)\n",
    "        return x\n",
    "\n",
    "class LinearWithLoRA(nn.Module):\n",
    "    def __init__(self, linear, rank, alpha):\n",
    "        super().__init__()\n",
    "        self.linear = linear\n",
    "        self.lora = LoRALayer(\n",
    "            linear.in_features, linear.out_features, rank, alpha\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x) + self.lora(x)\n",
    "\n",
    "def ReplaceLinearToLoRA(model, rank, alpha):\n",
    "    for name, module in model.named_children():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            setattr(model, name, LinearWithLoRA(module, rank=rank, alpha=alpha))\n",
    "        else:\n",
    "            ReplaceLinearToLoRA(module, rank=rank, alpha=alpha)\n",
    "\n",
    "def FreeazeModel(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "def UnfreezeLoRA(model):\n",
    "    for child in model.children():\n",
    "        if isinstance(child, LoRALayer):\n",
    "            for param in child.parameters():\n",
    "                param.requires_grad = True\n",
    "        else:\n",
    "            # Recursively freeze linear layers in children modules\n",
    "            UnfreezeLoRA(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=100, out_features=1000, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class MultilayerPerceptron(nn.Module):\n",
    "    def __init__(self, num_features, \n",
    "        num_hidden_1, num_hidden_2, num_classes):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(num_features, num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_1, num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(num_hidden_2, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model = MultilayerPerceptron(\n",
    "    num_features=100,\n",
    "    num_hidden_1=1000,\n",
    "    num_hidden_2=1000, \n",
    "    num_classes=10\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultilayerPerceptron(\n",
      "  (layers): Sequential(\n",
      "    (0): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=100, out_features=1000, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (1): ReLU()\n",
      "    (2): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "    (3): ReLU()\n",
      "    (4): LinearWithLoRA(\n",
      "      (linear): Linear(in_features=1000, out_features=10, bias=True)\n",
      "      (lora): LoRALayer()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "ReplaceLinearToLoRA(model, 4, 8)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreeazeModel(model)\n",
    "UnfreezeLoRA(model)\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ReplaceLinearToLoRA(vgg16_model, 4, 8)\n",
    "print(vgg16_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FreeazeModel(vgg16_model)\n",
    "UnfreezeLoRA(vgg16_model)\n",
    "for name, param in vgg16_model.named_parameters():\n",
    "    print(f\"{name}: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/4], Step [5000/5000], Loss: nan, Accuracy: 10.05%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [2/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [3/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n",
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Epoch [4/4], Step [5000/5000], Loss: nan, Accuracy: 10.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:05:09,003] Trial 0 failed with parameters: {'lora_alpha': 40, 'optimizer': 'SGD', 'lr': 0.021811288119580597, 'batch_size': 8, 'num_epochs': 4} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:05:09,004] Trial 0 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 9.88%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/3], Step [625/625], Loss: nan, Accuracy: 10.27%\n",
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Epoch [2/3], Step [625/625], Loss: nan, Accuracy: 10.08%\n",
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Epoch [3/3], Step [625/625], Loss: nan, Accuracy: 10.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:05:33,624] Trial 1 failed with parameters: {'lora_alpha': 24, 'optimizer': 'SGD', 'lr': 0.19912349713389108, 'batch_size': 64, 'num_epochs': 3} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:05:33,625] Trial 1 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 9.67%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/6], Step [625/625], Loss: nan, Accuracy: 10.62%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [2/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [3/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [4/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [5/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n",
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Epoch [6/6], Step [625/625], Loss: nan, Accuracy: 10.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:06:19,338] Trial 2 failed with parameters: {'lora_alpha': 48, 'optimizer': 'SGD', 'lr': 0.015138820653668743, 'batch_size': 64, 'num_epochs': 6} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:06:19,338] Trial 2 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 10.00%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 0.1714, Accuracy: 96.17%\n",
      "Val Loss: 0.1014, Val Accuracy: 98.30%\n",
      "Epoch [2/5], Step [625/625], Loss: 0.1740, Accuracy: 96.13%\n",
      "Val Loss: 0.6072, Val Accuracy: 83.27%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1705, Accuracy: 96.27%\n",
      "Val Loss: 0.0746, Val Accuracy: 98.60%\n",
      "Epoch [4/5], Step [625/625], Loss: 0.1159, Accuracy: 97.70%\n",
      "Val Loss: 0.1325, Val Accuracy: 97.53%\n",
      "Epoch [5/5], Step [625/625], Loss: 0.2009, Accuracy: 95.79%\n",
      "Val Loss: 1.3934, Val Accuracy: 69.71%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/4], Step [5000/5000], Loss: 2.2882, Accuracy: 73.38%\n",
      "Val Loss: 5.2116, Val Accuracy: 46.69%\n",
      "Epoch [2/4], Step [5000/5000], Loss: 3.0742, Accuracy: 74.94%\n",
      "Val Loss: 0.9900, Val Accuracy: 79.88%\n",
      "Epoch [3/4], Step [5000/5000], Loss: 4.1397, Accuracy: 74.81%\n",
      "Val Loss: 0.4905, Val Accuracy: 86.07%\n",
      "Epoch [4/4], Step [5000/5000], Loss: 2.8908, Accuracy: 76.80%\n",
      "Val Loss: 8.6441, Val Accuracy: 31.10%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/2], Step [5000/5000], Loss: 0.6368, Accuracy: 85.10%\n",
      "Val Loss: 0.2694, Val Accuracy: 92.80%\n",
      "Epoch [2/2], Step [5000/5000], Loss: 0.4458, Accuracy: 89.37%\n",
      "Val Loss: 0.1610, Val Accuracy: 97.74%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 1.3562, Accuracy: 88.12%\n",
      "Val Loss: 0.7727, Val Accuracy: 80.60%\n",
      "Epoch [2/5], Step [625/625], Loss: 0.1869, Accuracy: 96.03%\n",
      "Val Loss: 0.0759, Val Accuracy: 98.52%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1120, Accuracy: 97.62%\n",
      "Val Loss: 0.0873, Val Accuracy: 98.23%\n",
      "Epoch [4/5], Step [625/625], Loss: 3.7163, Accuracy: 85.12%\n",
      "Val Loss: 4.5051, Val Accuracy: 67.45%\n",
      "Epoch [5/5], Step [625/625], Loss: 0.3836, Accuracy: 94.20%\n",
      "Val Loss: 0.0671, Val Accuracy: 98.75%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/6], Step [1250/1250], Loss: 0.2938, Accuracy: 92.96%\n",
      "Val Loss: 0.1431, Val Accuracy: 97.57%\n",
      "Epoch [2/6], Step [1250/1250], Loss: 0.3498, Accuracy: 92.46%\n",
      "Val Loss: 0.1081, Val Accuracy: 98.10%\n",
      "Epoch [3/6], Step [1250/1250], Loss: 0.5047, Accuracy: 91.72%\n",
      "Val Loss: 0.0876, Val Accuracy: 98.48%\n",
      "Epoch [4/6], Step [1250/1250], Loss: 0.1556, Accuracy: 96.62%\n",
      "Val Loss: 0.1054, Val Accuracy: 98.28%\n",
      "Epoch [5/6], Step [1250/1250], Loss: 0.7857, Accuracy: 89.82%\n",
      "Val Loss: 0.1255, Val Accuracy: 97.84%\n",
      "Epoch [6/6], Step [1250/1250], Loss: 0.1423, Accuracy: 97.01%\n",
      "Val Loss: 0.0813, Val Accuracy: 98.57%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/7], Step [1250/1250], Loss: nan, Accuracy: 9.91%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [2/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [3/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [4/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [5/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [6/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n",
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Epoch [7/7], Step [1250/1250], Loss: nan, Accuracy: 9.82%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-17 15:11:41,656] Trial 8 failed with parameters: {'lora_alpha': 48, 'optimizer': 'SGD', 'lr': 0.0634383447468684, 'batch_size': 32, 'num_epochs': 7} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-03-17 15:11:41,657] Trial 8 failed with value nan.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss: nan, Val Accuracy: 10.71%\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch [1/5], Step [625/625], Loss: 13.6492, Accuracy: 53.09%\n",
      "Val Loss: 2.0995, Val Accuracy: 76.05%\n",
      "Epoch [2/5], Step [625/625], Loss: 5.9190, Accuracy: 60.47%\n",
      "Val Loss: 0.3330, Val Accuracy: 92.08%\n",
      "Epoch [3/5], Step [625/625], Loss: 0.1708, Accuracy: 96.05%\n",
      "Val Loss: 0.0850, Val Accuracy: 98.36%\n",
      "Epoch [4/5], Step [625/625], Loss: 0.1408, Accuracy: 96.97%\n",
      "Val Loss: 0.0874, Val Accuracy: 98.60%\n",
      "Epoch [5/5], Step [625/625], Loss: 58.5076, Accuracy: 82.99%\n",
      "Val Loss: 41.9884, Val Accuracy: 65.85%\n",
      "Study statistics: \n",
      " Number of finished trials:  10\n",
      " Number of pruned trials:  0\n",
      " Number of complete trials:  6\n",
      "Best trial:\n",
      " Value:  41.98835474974031\n",
      " Params: \n",
      " lora_alpha: 48\n",
      " optimizer: Adam\n",
      " lr: 0.16628497525202182\n",
      " batch_size: 64\n",
      " num_epochs: 5\n"
     ]
    }
   ],
   "source": [
    "from train_evaluate.train_model import *\n",
    "study = optuna_trials()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For 'mean' quant, 'rank'=10:\n",
    "\n",
    "Study statistics:\n",
    " Number of finished trials:  10\n",
    " Number of pruned trials:  0\n",
    " Number of complete trials:  9\n",
    "Best trial:\n",
    " Value:  29.649906891629023\n",
    " Params:\n",
    " lora_alpha: 32\n",
    " optimizer: Adam\n",
    " lr: 0.08913593530369923\n",
    " batch_size: 32\n",
    " num_epochs: 5"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "9"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "['vgg16_bn_cifar10', 'vgg16_bn_cifar100']"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in timm.list_models(pretrained=True) if 'vgg' in i and 'cifar' in i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/edadaltocg/vgg16_bn_cifar10/resolve/main/pytorch_model.bin\" to /home/ohada/.cache/torch/hub/checkpoints/vgg16_bn_cifar10.pth\n",
      "100%|| 56.2M/56.2M [00:02<00:00, 29.1MB/s]\n"
     ]
    }
   ],
   "source": [
    "m_vgg = timm.create_model(\"vgg16_bn_cifar10\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "['repvgg_a0.rvgg_in1k',\n 'repvgg_a1.rvgg_in1k',\n 'repvgg_a2.rvgg_in1k',\n 'repvgg_b0.rvgg_in1k',\n 'repvgg_b1.rvgg_in1k',\n 'repvgg_b1g4.rvgg_in1k',\n 'repvgg_b2.rvgg_in1k',\n 'repvgg_b2g4.rvgg_in1k',\n 'repvgg_b3.rvgg_in1k',\n 'repvgg_b3g4.rvgg_in1k',\n 'repvgg_d2se.rvgg_in1k',\n 'vgg11.tv_in1k',\n 'vgg11_bn.tv_in1k',\n 'vgg13.tv_in1k',\n 'vgg13_bn.tv_in1k',\n 'vgg16.tv_in1k',\n 'vgg16_bn.tv_in1k',\n 'vgg16_bn_cifar10',\n 'vgg16_bn_cifar100',\n 'vgg16_bn_svhn',\n 'vgg19.tv_in1k',\n 'vgg19_bn.tv_in1k']"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in timm.list_models(pretrained=True) if 'vgg' in i]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://huggingface.co/edadaltocg/densenet121_cifar10/resolve/main/pytorch_model.bin\" to /home/ohada/.cache/torch/hub/checkpoints/densenet121_cifar10.pth\n",
      "100%|| 4.19M/4.19M [00:00<00:00, 6.34MB/s]\n"
     ]
    }
   ],
   "source": [
    "m_dense = timm.create_model(\"densenet121_cifar10\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/553M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2570473ccdd343c0a4b43bb8e9a99caf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count the number of 'Linear' layers in the model:\n",
    "def count_linear_layers(model):\n",
    "    count = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Linear):\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "count_linear_layers(timm.create_model(\"vgg16.tv_in1k\", pretrained=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "m_vgg_1k = timm.create_model(\"vgg16.tv_in1k\", pretrained=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "VGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (pre_logits): ConvMlp(\n    (fc1): Conv2d(512, 4096, kernel_size=(7, 7), stride=(1, 1))\n    (act1): ReLU(inplace=True)\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc2): Conv2d(4096, 4096, kernel_size=(1, 1), stride=(1, 1))\n    (act2): ReLU(inplace=True)\n  )\n  (head): ClassifierHead(\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (drop): Dropout(p=0.0, inplace=False)\n    (fc): Linear(in_features=4096, out_features=1000, bias=True)\n    (flatten): Identity()\n  )\n)"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
